"""
Hybrid Response Generator - AIã¨æ—¢å­˜ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å¿œç­”
"""
import logging
import asyncio
from typing import Dict, Optional, Any, List, Union, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
import threading
import json

from .flexible_ai_service import flexible_ai_service
from .service_integration_manager import service_integration_manager
from .context_aware_router import context_aware_router, RoutingDecision
from .adaptive_prompt_manager import adaptive_prompt_manager

@dataclass
class HybridResponseComponent:
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å¿œç­”ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"""
    component_type: str  # ai, service, combined
    service_name: str
    content: str
    confidence: float
    metadata: Dict[str, Any]
    priority: int = 5

@dataclass
class HybridResponse:
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å¿œç­”"""
    response_id: str
    user_id: str
    original_query: str
    components: List[HybridResponseComponent]
    final_response: str
    routing_decision: RoutingDecision
    processing_time: float
    quality_score: float
    user_feedback: Optional[Dict[str, Any]] = None

class HybridResponseGenerator:
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å¿œç­”ç”Ÿæˆå™¨"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

        # å¿œç­”çµ±åˆãƒ«ãƒ¼ãƒ«
        self.integration_rules = self._initialize_integration_rules()

        # å¿œç­”å“è³ªè©•ä¾¡åŸºæº–
        self.quality_metrics = self._initialize_quality_metrics()

        # å¿œç­”ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.response_templates = self._initialize_response_templates()

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.response_cache: Dict[str, HybridResponse] = {}
        self.cache_lock = threading.Lock()

        self.logger.info("Hybrid Response Generatorã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")

    def _initialize_integration_rules(self) -> Dict[str, List[Dict[str, Any]]]:
        """çµ±åˆãƒ«ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–"""
        return {
            "notification": [
                {
                    "condition": "single_notification",
                    "template": "notification_confirmation",
                    "priority": 10
                },
                {
                    "condition": "multiple_notifications",
                    "template": "notification_summary",
                    "priority": 8
                }
            ],
            "weather": [
                {
                    "condition": "weather_with_location",
                    "template": "weather_detailed",
                    "priority": 9
                },
                {
                    "condition": "weather_forecast",
                    "template": "weather_forecast",
                    "priority": 8
                }
            ],
            "search": [
                {
                    "condition": "search_with_ai_summary",
                    "template": "search_with_ai_insights",
                    "priority": 9
                },
                {
                    "condition": "simple_search",
                    "template": "search_results",
                    "priority": 7
                }
            ],
            "auto_task": [
                {
                    "condition": "task_creation",
                    "template": "task_confirmation",
                    "priority": 10
                },
                {
                    "condition": "task_management",
                    "template": "task_management",
                    "priority": 8
                }
            ],
            "combined": [
                {
                    "condition": "multi_service",
                    "template": "combined_services",
                    "priority": 9
                }
            ]
        }

    def _initialize_quality_metrics(self) -> Dict[str, Dict[str, float]]:
        """å“è³ªè©•ä¾¡åŸºæº–ã‚’åˆæœŸåŒ–"""
        return {
            "completeness": {
                "min_length": 10,
                "has_details": 1.0,
                "structured": 0.5,
                "weight": 0.3
            },
            "relevance": {
                "keyword_match": 1.0,
                "context_appropriate": 0.8,
                "user_intent_fulfilled": 1.0,
                "weight": 0.3
            },
            "accuracy": {
                "factual_correctness": 1.0,
                "up_to_date": 0.5,
                "consistent": 0.5,
                "weight": 0.2
            },
            "usability": {
                "actionable": 0.8,
                "clear_instructions": 0.5,
                "follow_up_options": 0.3,
                "weight": 0.2
            }
        }

    def _initialize_response_templates(self) -> Dict[str, str]:
        """å¿œç­”ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’åˆæœŸåŒ–"""
        return {
            "notification_confirmation": """
âœ… é€šçŸ¥ã‚’è¨­å®šã—ã¾ã—ãŸ

{notification_details}

ğŸ†” é€šçŸ¥ID: {notification_id}
â° å®Ÿè¡Œäºˆå®š: {schedule_time}
ğŸ”„ ç¹°ã‚Šè¿”ã—: {repeat_pattern}

å¤‰æ›´ãŒå¿…è¦ãªå ´åˆã¯ã€Œé€šçŸ¥ç·¨é›† {notification_id}ã€ã¨ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚
            """.strip(),

            "notification_summary": """
ğŸ“‹ é€šçŸ¥ä¸€è¦§

{multiple_notifications}

åˆè¨ˆ: {total_count} ä»¶ã®é€šçŸ¥ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚
            """.strip(),

            "weather_detailed": """
ğŸŒ¤ï¸ {location}ã®å¤©æ°—æƒ…å ±

{today_weather}

{tomorrow_forecast}

ğŸ’¡ ä»Šæ—¥ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹: {weather_advice}
            """.strip(),

            "weather_forecast": """
ğŸ“… {location}ã®å¤©æ°—äºˆå ±

{forecast_details}

æ¬¡å›ã®å¤©æ°—ç¢ºèªã¯ã€Œ{location}ã®å¤©æ°—ã€ã¨ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚
            """.strip(),

            "search_with_ai_insights": """
ğŸ” æ¤œç´¢çµæœ: "{query}"

{search_results}

ğŸ¤– AIã«ã‚ˆã‚‹è¿½åŠ åˆ†æ:
{ai_insights}
            """.strip(),

            "search_results": """
ğŸ” æ¤œç´¢çµæœ: "{query}"

{search_results}

ã‚ˆã‚Šè©³ã—ã„æƒ…å ±ãŒå¿…è¦ã§ã™ã‹ï¼Ÿ
            """.strip(),

            "task_confirmation": """
âœ… è‡ªå‹•ã‚¿ã‚¹ã‚¯ã‚’è¨­å®šã—ã¾ã—ãŸ

ğŸ“ ã‚¿ã‚¹ã‚¯: {task_title}
â° å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°: {schedule}
ğŸ”„ ç¹°ã‚Šè¿”ã—: {repeat_pattern}
ğŸ†” ã‚¿ã‚¹ã‚¯ID: {task_id}

ç®¡ç†ã¯ã€Œè‡ªå‹•å®Ÿè¡Œä¸€è¦§ã€ã§ç¢ºèªã§ãã¾ã™ã€‚
            """.strip(),

            "task_management": """
ğŸ¤– è‡ªå‹•ã‚¿ã‚¹ã‚¯ç®¡ç†

{task_management_info}

ä»–ã®ã‚¿ã‚¹ã‚¯ã‚‚è¨­å®šã—ã¾ã™ã‹ï¼Ÿ
            """.strip(),

            "combined_services": """
ğŸ”„ è¤‡æ•°ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ãŸç·åˆçš„ãªå¯¾å¿œ

{service_integration_details}

è¿½åŠ ã®å¯¾å¿œãŒå¿…è¦ã§ã™ã‹ï¼Ÿ
            """.strip(),

            "ai_enhanced_response": """
{ai_response}

ğŸ’¡ ã“ã®å›ç­”ã¯AIã®åˆ†æã¨æ—¢å­˜ã®ã‚µãƒ¼ãƒ“ã‚¹æ©Ÿèƒ½ã‚’çµ„ã¿åˆã‚ã›ã¦ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚
            """.strip(),

            "general_assistance": """
{primary_response}

{additional_suggestions}

ã”è³ªå•ãŒã‚ã‚Œã°ã„ã¤ã§ã‚‚ã©ã†ãï¼
            """.strip()
        }

    async def generate_hybrid_response(
        self,
        query: str,
        user_id: str = "default",
        context: Optional[Dict[str, Any]] = None
    ) -> HybridResponse:
        """
        ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å¿œç­”ã‚’ç”Ÿæˆ

        Args:
            query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            context: è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å¿œç­”
        """
        start_time = datetime.now()

        try:
            # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ±ºå®šã‚’å–å¾—
            routing_decision = await context_aware_router.analyze_and_route(
                query, user_id, context
            )

            # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆç”Ÿæˆ
            components = await self._generate_response_components(
                query, routing_decision, context or {}
            )

            # æœ€çµ‚å¿œç­”çµ±åˆ
            final_response = await self._integrate_response_components(
                components, routing_decision
            )

            # å“è³ªè©•ä¾¡
            quality_score = self._evaluate_response_quality(
                final_response, components, routing_decision
            )

            # å¿œç­”IDç”Ÿæˆ
            response_id = f"hybrid_{datetime.now().strftime('%Y%m%d%H%M%S')}_{user_id[:4]}"

            hybrid_response = HybridResponse(
                response_id=response_id,
                user_id=user_id,
                original_query=query,
                components=components,
                final_response=final_response,
                routing_decision=routing_decision,
                processing_time=(datetime.now() - start_time).total_seconds(),
                quality_score=quality_score
            )

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            self._cache_response(hybrid_response)

            return hybrid_response

        except Exception as e:
            self.logger.error(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return self._create_error_response(query, user_id, str(e))

    async def _generate_response_components(
        self,
        query: str,
        routing_decision: RoutingDecision,
        context: Dict[str, Any]
    ) -> List[HybridResponseComponent]:
        """å¿œç­”ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ç”Ÿæˆ"""
        components = []

        try:
            # ä¸»è¦ã‚µãƒ¼ãƒ“ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
            primary_component = await self._generate_service_component(
                routing_decision.selected_service,
                routing_decision.selected_method,
                routing_decision.execution_parameters,
                context
            )

            if primary_component:
                components.append(primary_component)

            # å‰¯æ¬¡çš„ã‚µãƒ¼ãƒ“ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
            for service in routing_decision.analysis.secondary_services:
                secondary_component = await self._generate_service_component(
                    service, "default", {}, context
                )
                if secondary_component:
                    components.append(secondary_component)

            # AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆå¿…è¦ãªå ´åˆï¼‰
            if routing_decision.ai_enhanced:
                ai_component = await self._generate_ai_component(
                    query, routing_decision, context
                )
                if ai_component:
                    components.append(ai_component)

        except Exception as e:
            self.logger.error(f"ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
            fallback_component = HybridResponseComponent(
                component_type="service",
                service_name="fallback",
                content="ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
                confidence=0.0,
                metadata={"error": str(e)}
            )
            components.append(fallback_component)

        return components

    async def _generate_service_component(
        self,
        service_name: str,
        method: str,
        parameters: Dict[str, Any],
        context: Dict[str, Any]
    ) -> Optional[HybridResponseComponent]:
        """ã‚µãƒ¼ãƒ“ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ç”Ÿæˆ"""
        try:
            # ã‚µãƒ¼ãƒ“ã‚¹çµ±åˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’é€šã˜ã¦å®Ÿè¡Œ
            result = await self._execute_service_method(service_name, method, parameters, context)

            if result and result.get("success", False):
                return HybridResponseComponent(
                    component_type="service",
                    service_name=service_name,
                    content=self._format_service_result(service_name, result),
                    confidence=0.9,
                    metadata=result,
                    priority=8
                )

        except Exception as e:
            self.logger.warning(f"ã‚µãƒ¼ãƒ“ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼ ({service_name}): {str(e)}")

        return None

    async def _execute_service_method(
        self,
        service_name: str,
        method: str,
        parameters: Dict[str, Any],
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ã‚µãƒ¼ãƒ“ã‚¹ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè¡Œ"""
        # å®Ÿéš›ã®ã‚µãƒ¼ãƒ“ã‚¹å®Ÿè¡Œ
        # ã“ã“ã§ã¯ãƒ¢ãƒƒã‚¯å®Ÿè£…
        if service_name == "notification":
            return await self._mock_notification_service(method, parameters, context)
        elif service_name == "weather":
            return await self._mock_weather_service(method, parameters, context)
        elif service_name == "search":
            return await self._mock_search_service(method, parameters, context)
        elif service_name == "auto_task":
            return await self._mock_auto_task_service(method, parameters, context)
        else:
            return {"success": False, "error": "Unknown service"}

    async def _mock_notification_service(self, method: str, parameters: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """é€šçŸ¥ã‚µãƒ¼ãƒ“ã‚¹ãƒ¢ãƒƒã‚¯"""
        if method == "add":
            return {
                "success": True,
                "notification_id": f"n_{datetime.now().strftime('%Y%m%d%H%M%S')}",
                "title": parameters.get("title", "é€šçŸ¥"),
                "message": parameters.get("message", ""),
                "schedule": parameters.get("datetime", ""),
                "created_at": datetime.now().isoformat()
            }
        elif method == "list":
            return {
                "success": True,
                "notifications": [
                    {"id": "n1", "title": "ãƒ†ã‚¹ãƒˆé€šçŸ¥1", "message": "ãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸1"},
                    {"id": "n2", "title": "ãƒ†ã‚¹ãƒˆé€šçŸ¥2", "message": "ãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸2"}
                ]
            }
        return {"success": False}

    async def _mock_weather_service(self, method: str, parameters: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """å¤©æ°—ã‚µãƒ¼ãƒ“ã‚¹ãƒ¢ãƒƒã‚¯"""
        location = parameters.get("location", "æ±äº¬")
        return {
            "success": True,
            "location": location,
            "temperature": "22Â°C",
            "condition": "æ™´ã‚Œ",
            "humidity": "60%",
            "wind_speed": "3m/s",
            "retrieved_at": datetime.now().isoformat()
        }

    async def _mock_search_service(self, method: str, parameters: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """æ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ãƒ¢ãƒƒã‚¯"""
        query = parameters.get("query", "")
        return {
            "success": True,
            "query": query,
            "results": [
                f"{query}ã®æ¤œç´¢çµæœ1",
                f"{query}ã®æ¤œç´¢çµæœ2",
                f"{query}ã®æ¤œç´¢çµæœ3"
            ],
            "total_results": 3,
            "search_time": datetime.now().isoformat()
        }

    async def _mock_auto_task_service(self, method: str, parameters: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """è‡ªå‹•ã‚¿ã‚¹ã‚¯ã‚µãƒ¼ãƒ“ã‚¹ãƒ¢ãƒƒã‚¯"""
        if method == "create":
            return {
                "success": True,
                "task_id": f"t_{datetime.now().strftime('%Y%m%d%H%M%S')}",
                "title": parameters.get("title", "ã‚¿ã‚¹ã‚¯"),
                "schedule": parameters.get("schedule", ""),
                "status": "active",
                "created_at": datetime.now().isoformat()
            }
        elif method == "list":
            return {
                "success": True,
                "tasks": [
                    {"id": "t1", "title": "ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯1", "status": "active"},
                    {"id": "t2", "title": "ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯2", "status": "inactive"}
                ]
            }
        return {"success": False}

    def _format_service_result(self, service_name: str, result: Dict[str, Any]) -> str:
        """ã‚µãƒ¼ãƒ“ã‚¹çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        if service_name == "notification":
            return f"é€šçŸ¥: {result.get('title', '')} - {result.get('message', '')}"
        elif service_name == "weather":
            return f"{result.get('location', '')}ã®å¤©æ°—: {result.get('condition', '')} {result.get('temperature', '')}"
        elif service_name == "search":
            results = result.get('results', [])
            return f"æ¤œç´¢çµæœ: {' | '.join(results[:2])}"
        elif service_name == "auto_task":
            return f"ã‚¿ã‚¹ã‚¯: {result.get('title', '')} - {result.get('status', '')}"
        else:
            return str(result)

    async def _generate_ai_component(
        self,
        query: str,
        routing_decision: RoutingDecision,
        context: Dict[str, Any]
    ) -> Optional[HybridResponseComponent]:
        """AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ç”Ÿæˆ"""
        try:
            # AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
            ai_prompt = await self._generate_ai_enhancement_prompt(
                query, routing_decision, context
            )

            # AIå¿œç­”ç”Ÿæˆ
            ai_response = await flexible_ai_service.generate_flexible_response(
                ai_prompt,
                context={"hybrid_ai": True, "routing_decision": asdict(routing_decision)}
            )

            return HybridResponseComponent(
                component_type="ai",
                service_name="flexible_ai",
                content=ai_response,
                confidence=0.8,
                metadata={
                    "prompt": ai_prompt,
                    "ai_model": "gemini",
                    "generated_at": datetime.now().isoformat()
                },
                priority=7
            )

        except Exception as e:
            self.logger.warning(f"AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None

    async def _generate_ai_enhancement_prompt(
        self,
        query: str,
        routing_decision: RoutingDecision,
        context: Dict[str, Any]
    ) -> str:
        """AIå¼·åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        service_context = service_integration_manager.get_service_capability(
            routing_decision.selected_service
        )

        prompt = f"""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦ã€æ—¢å­˜ã®{routing_decision.selected_service}ã‚µãƒ¼ãƒ“ã‚¹ã¨é€£æºã—ãŸå›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

        ã‚¯ã‚¨ãƒª: {query}

        é¸æŠã•ã‚ŒãŸã‚µãƒ¼ãƒ“ã‚¹: {routing_decision.selected_service}
        ã‚µãƒ¼ãƒ“ã‚¹æƒ…å ±: {service_context.description if service_context else "æƒ…å ±ãªã—"}

        ã‚ãªãŸã®å½¹å‰²:
        - ã‚µãƒ¼ãƒ“ã‚¹ã®æ©Ÿèƒ½ã‚’è£œå®Œãƒ»å¼·åŒ–ã™ã‚‹
        - ã‚ˆã‚Šãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸå¯¾å¿œã‚’æä¾›
        - è¿½åŠ ã®æ´å¯Ÿã‚„ææ¡ˆã‚’åŠ ãˆã‚‹
        - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä½“é¨“ã‚’å‘ä¸Šã•ã›ã‚‹

        å›ç­”ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³:
        - ã‚µãƒ¼ãƒ“ã‚¹ã®åŸºæœ¬æ©Ÿèƒ½ã‚’å°Šé‡
        - è¿½åŠ ä¾¡å€¤ã‚’æä¾›
        - æ˜ç¢ºã§å½¹ç«‹ã¤æƒ…å ±ã‚’å«ã‚ã‚‹
        - éåº¦ã«è¤‡é›‘ã«ã—ãªã„

        å›ç­”:
        """

        return prompt.strip()

    async def _integrate_response_components(
        self,
        components: List[HybridResponseComponent],
        routing_decision: RoutingDecision
    ) -> str:
        """å¿œç­”ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’çµ±åˆ"""
        if not components:
            return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€é©åˆ‡ãªå¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

        # å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆ
        components.sort(key=lambda c: c.priority, reverse=True)

        # çµ±åˆãƒ«ãƒ¼ãƒ«ã«åŸºã¥ã„ã¦ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé¸æŠ
        integration_type = self._determine_integration_type(components, routing_decision)

        # æœ€é©ãªçµ±åˆæ–¹æ³•ã‚’é¸æŠ
        integration_method = self._select_integration_method(
            integration_type, routing_decision
        )

        # çµ±åˆå®Ÿè¡Œ
        if integration_method == "template_based":
            return await self._integrate_with_template(components, routing_decision, integration_type)
        elif integration_method == "ai_powered":
            return await self._integrate_with_ai(components, routing_decision)
        else:
            # ã‚·ãƒ³ãƒ—ãƒ«çµ±åˆ
            return self._integrate_simple(components)

    def _determine_integration_type(
        self,
        components: List[HybridResponseComponent],
        routing_decision: RoutingDecision
    ) -> str:
        """çµ±åˆã‚¿ã‚¤ãƒ—ã‚’æ±ºå®š"""
        if len(components) == 1:
            return routing_decision.selected_service
        else:
            return "combined"

    def _select_integration_method(
        self,
        integration_type: str,
        routing_decision: RoutingDecision
    ) -> str:
        """çµ±åˆæ–¹æ³•ã‚’é¸æŠ"""
        if routing_decision.ai_enhanced:
            return "ai_powered"
        elif integration_type in self.integration_rules:
            return "template_based"
        else:
            return "simple"

    async def _integrate_with_template(
        self,
        components: List[HybridResponseComponent],
        routing_decision: RoutingDecision,
        integration_type: str
    ) -> str:
        """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹çµ±åˆ"""
        template_name = self._get_best_template(integration_type, components)

        if template_name and template_name in self.response_templates:
            template = self.response_templates[template_name]

            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ•°ã‚’ç½®æ›
            formatted_response = template

            # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå†…å®¹ã®æŒ¿å…¥
            for component in components:
                if component.service_name in routing_decision.selected_service:
                    formatted_response = formatted_response.replace(
                        f"{{{component.service_name}_details}}",
                        component.content
                    )

            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®è¨­å®š
            formatted_response = formatted_response.replace("{query}", routing_decision.original_query)
            formatted_response = formatted_response.replace("{notification_id}", "N/A")
            formatted_response = formatted_response.replace("{schedule_time}", "æŒ‡å®šãªã—")
            formatted_response = formatted_response.replace("{repeat_pattern}", "ãªã—")

            return formatted_response

        return self._integrate_simple(components)

    async def _integrate_with_ai(
        self,
        components: List[HybridResponseComponent],
        routing_decision: RoutingDecision
    ) -> str:
        """AIã‚’æ´»ç”¨ã—ãŸçµ±åˆ"""
        # çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        integration_prompt = self._generate_integration_prompt(components, routing_decision)

        try:
            # AIã«ã‚ˆã‚‹çµ±åˆå¿œç­”ç”Ÿæˆ
            ai_response = await flexible_ai_service.generate_flexible_response(
                integration_prompt,
                context={"response_integration": True}
            )

            return ai_response

        except Exception as e:
            self.logger.warning(f"AIçµ±åˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return self._integrate_simple(components)

    def _generate_integration_prompt(
        self,
        components: List[HybridResponseComponent],
        routing_decision: RoutingDecision
    ) -> str:
        """çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        components_text = "\n".join([
            f"- {c.service_name}: {c.content}"
            for c in components
        ])

        prompt = f"""
        è¤‡æ•°ã®ã‚µãƒ¼ãƒ“ã‚¹ã‹ã‚‰ã®å¿œç­”ã‚’çµ±åˆã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æœ€é©ãªå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

        å…ƒã®ã‚¯ã‚¨ãƒª: {routing_decision.original_query}

        ã‚µãƒ¼ãƒ“ã‚¹ã‹ã‚‰ã®å¿œç­”:
        {components_text}

        çµ±åˆã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³:
        - å„ã‚µãƒ¼ãƒ“ã‚¹ã®æƒ…å ±ã‚’é©åˆ‡ã«çµ„ã¿åˆã‚ã›ã‚‹
        - é‡è¤‡ã‚’é¿ã‘ã€ç°¡æ½”ã«ã¾ã¨ã‚ã‚‹
        - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ©ç›Šã«ãªã‚‹ã‚ˆã†æ•´ç†
        - è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„å½¢å¼ã«ã™ã‚‹

        çµ±åˆã•ã‚ŒãŸå›ç­”:
        """

        return prompt.strip()

    def _integrate_simple(self, components: List[HybridResponseComponent]) -> str:
        """ã‚·ãƒ³ãƒ—ãƒ«çµ±åˆ"""
        if not components:
            return "å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

        # æœ€é«˜å„ªå…ˆåº¦ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ä½¿ç”¨
        best_component = max(components, key=lambda c: c.priority)
        return best_component.content

    def _get_best_template(
        self,
        integration_type: str,
        components: List[HybridResponseComponent]
    ) -> Optional[str]:
        """æœ€é©ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—"""
        rules = self.integration_rules.get(integration_type, [])

        if not rules:
            return None

        # æ¡ä»¶ã«ãƒãƒƒãƒã™ã‚‹ãƒ«ãƒ¼ãƒ«ã‚’æ¢ã™
        for rule in rules:
            if self._check_integration_condition(rule, components):
                return rule.get("template")

        return None

    def _check_integration_condition(self, rule: Dict[str, Any], components: List[HybridResponseComponent]) -> bool:
        """çµ±åˆæ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯"""
        condition = rule.get("condition", "")

        if condition == "single_notification" and len(components) == 1:
            return components[0].service_name == "notification"

        elif condition == "multiple_notifications" and len(components) > 1:
            return all(c.service_name == "notification" for c in components)

        elif condition == "weather_with_location":
            return any("location" in c.content.lower() for c in components)

        elif condition == "search_with_ai_summary":
            return any(c.component_type == "ai" for c in components)

        return True

    def _evaluate_response_quality(
        self,
        final_response: str,
        components: List[HybridResponseComponent],
        routing_decision: RoutingDecision
    ) -> float:
        """å¿œç­”å“è³ªã‚’è©•ä¾¡"""
        total_score = 0.0
        total_weight = 0.0

        for metric, criteria in self.quality_metrics.items():
            weight = criteria.get("weight", 0.2)

            # å„è©•ä¾¡åŸºæº–ã«åŸºã¥ã„ã¦ã‚¹ã‚³ã‚¢è¨ˆç®—
            if metric == "completeness":
                score = self._evaluate_completeness(final_response, criteria)
            elif metric == "relevance":
                score = self._evaluate_relevance(final_response, routing_decision, criteria)
            elif metric == "accuracy":
                score = self._evaluate_accuracy(components, criteria)
            elif metric == "usability":
                score = self._evaluate_usability(final_response, criteria)
            else:
                score = 0.5

            total_score += score * weight
            total_weight += weight

        return total_score / total_weight if total_weight > 0 else 0.0

    def _evaluate_completeness(self, response: str, criteria: Dict[str, Any]) -> float:
        """å®Œå…¨æ€§ã‚’è©•ä¾¡"""
        score = 0.5

        # é•·ã•ãƒã‚§ãƒƒã‚¯
        min_length = criteria.get("min_length", 10)
        if len(response) >= min_length:
            score += 0.2

        # è©³ç´°æƒ…å ±ã®æœ‰ç„¡
        if criteria.get("has_details", 0):
            if len(response.split('\n')) > 2:
                score += 0.2

        # æ§‹é€ åŒ–
        if criteria.get("structured", 0):
            if any(char in response for char in ['â€¢', '-', '*', '1.', '2.']):
                score += 0.1

        return min(1.0, score)

    def _evaluate_relevance(self, response: str, routing_decision: RoutingDecision, criteria: Dict[str, Any]) -> float:
        """é–¢é€£æ€§ã‚’è©•ä¾¡"""
        score = 0.5

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ
        query_keywords = routing_decision.original_query.split()
        response_keywords = response.split()

        if criteria.get("keyword_match", 0):
            matches = sum(1 for kw in query_keywords if kw in response)
            if matches > 0:
                score += 0.2

        # æ–‡è„ˆé©åˆæ€§
        if criteria.get("context_appropriate", 0):
            if routing_decision.selected_service in response.lower():
                score += 0.2

        # æ„å›³å……è¶³
        if criteria.get("user_intent_fulfilled", 0):
            intent_type = routing_decision.analysis.intent_type
            if intent_type in response.lower():
                score += 0.1

        return min(1.0, score)

    def _evaluate_accuracy(self, components: List[HybridResponseComponent], criteria: Dict[str, Any]) -> float:
        """æ­£ç¢ºæ€§ã‚’è©•ä¾¡"""
        score = 0.5

        # äº‹å®Ÿã®æ­£ç¢ºæ€§ï¼ˆã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ä¿¡é ¼åº¦ã«åŸºã¥ãï¼‰
        if criteria.get("factual_correctness", 0):
            avg_confidence = sum(c.confidence for c in components) / len(components)
            score += (avg_confidence - 0.5) * 0.4

        # ä¸€è²«æ€§
        if criteria.get("consistent", 0):
            if len(components) > 1:
                score += 0.1

        return min(1.0, score)

    def _evaluate_usability(self, response: str, criteria: Dict[str, Any]) -> float:
        """ä½¿ã„ã‚„ã™ã•ã‚’è©•ä¾¡"""
        score = 0.5

        # å®Ÿè¡Œå¯èƒ½æ€§
        if criteria.get("actionable", 0):
            if any(word in response.lower() for word in ["è¨­å®š", "å®Ÿè¡Œ", "ç¢ºèª", "ä½¿ç”¨"]):
                score += 0.2

        # æ˜ç¢ºãªæŒ‡ç¤º
        if criteria.get("clear_instructions", 0):
            if any(word in response for word in ["ï¼š", "ãƒ»", "â†’", "ã‚¹ãƒ†ãƒƒãƒ—"]):
                score += 0.2

        # ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        if criteria.get("follow_up_options", 0):
            if any(word in response for word in ["ä»–ã«", "è¿½åŠ ", "ã•ã‚‰ã«", "è³ªå•"]):
                score += 0.1

        return min(1.0, score)

    def _create_error_response(self, query: str, user_id: str, error: str) -> HybridResponse:
        """ã‚¨ãƒ©ãƒ¼å¿œç­”ã‚’ä½œæˆ"""
        return HybridResponse(
            response_id=f"error_{datetime.now().strftime('%Y%m%d%H%M%S')}",
            user_id=user_id,
            original_query=query,
            components=[
                HybridResponseComponent(
                    component_type="service",
                    service_name="error",
                    content=f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error}",
                    confidence=0.0,
                    metadata={"error": error}
                )
            ],
            final_response=f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error}",
            routing_decision=None,
            processing_time=0.0,
            quality_score=0.0
        )

    def _cache_response(self, response: HybridResponse):
        """å¿œç­”ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        with self.cache_lock:
            self.response_cache[response.response_id] = response

    def get_response_statistics(self, user_id: str) -> Dict[str, Any]:
        """å¿œç­”çµ±è¨ˆã‚’å–å¾—"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰çµ±è¨ˆã‚’è¨ˆç®—
        user_responses = [
            r for r in self.response_cache.values()
            if r.user_id == user_id
        ]

        if not user_responses:
            return {"total_responses": 0, "average_quality": 0.0}

        total_quality = sum(r.quality_score for r in user_responses)
        avg_quality = total_quality / len(user_responses)

        service_usage = {}
        for response in user_responses:
            for component in response.components:
                service = component.service_name
                service_usage[service] = service_usage.get(service, 0) + 1

        return {
            "total_responses": len(user_responses),
            "average_quality": avg_quality,
            "service_usage": service_usage,
            "average_processing_time": sum(r.processing_time for r in user_responses) / len(user_responses)
        }

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
hybrid_response_generator = HybridResponseGenerator()
