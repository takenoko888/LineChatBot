"""
Flexible AI Service - æŸ”è»Ÿæ€§ã®é«˜ã„AIæ©Ÿèƒ½ã‚’æä¾›
"""
import logging
import json
import os
import asyncio
import random
from abc import ABC, abstractmethod
from typing import Dict, Optional, Any, List, Union, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
import threading
from concurrent.futures import ThreadPoolExecutor
import hashlib

# ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
@dataclass
class AIProviderConfig:
    """AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š"""
    name: str
    api_key_env: str
    base_url: str
    model_name: str
    max_tokens: int = 4000
    temperature: float = 0.7
    timeout: int = 30
    rate_limit_rpm: int = 60

class BaseAIProvider(ABC):
    """AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""

    def __init__(self, config: AIProviderConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.{config.name}")

    @abstractmethod
    async def generate_async(self, prompt: str, **kwargs) -> str:
        """éåŒæœŸç”Ÿæˆ"""
        pass

    @abstractmethod
    def generate_sync(self, prompt: str, **kwargs) -> str:
        """åŒæœŸç”Ÿæˆ"""
        pass

class GeminiProvider(BaseAIProvider):
    """Geminiãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""

    def __init__(self, config: AIProviderConfig):
        super().__init__(config)
        try:
            import google.generativeai as genai
            api_key = os.getenv(config.api_key_env)
            if not api_key:
                raise ValueError(f"{config.api_key_env}ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel(config.model_name)
            self.logger.info(f"{config.name}ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
        except ImportError:
            raise ImportError("google-generativeaiãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        except Exception as e:
            self.logger.error(f"Geminiãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise

    async def generate_async(self, prompt: str, **kwargs) -> str:
        """Geminiã§ã®éåŒæœŸç”Ÿæˆ"""
        try:
            # éåŒæœŸå‡¦ç†ã®ãŸã‚ã®ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ä½¿ç”¨
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as executor:
                response = await loop.run_in_executor(
                    executor,
                    self._generate_sync,
                    prompt,
                    kwargs
                )
                return response
        except Exception as e:
            self.logger.error(f"GeminiéåŒæœŸç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise

    def generate_sync(self, prompt: str, **kwargs) -> str:
        """Geminiã§ã®åŒæœŸç”Ÿæˆ"""
        return self._generate_sync(prompt, kwargs)

    def _generate_sync(self, prompt: str, kwargs: dict) -> str:
        """å†…éƒ¨åŒæœŸç”Ÿæˆå‡¦ç†"""
        try:
            # è¨­å®šã®ãƒãƒ¼ã‚¸
            config = {**self.config.__dict__, **kwargs}
            generation_config = {
                'temperature': config.get('temperature', self.config.temperature),
                'max_output_tokens': config.get('max_tokens', self.config.max_tokens),
            }

            response = self.model.generate_content(
                prompt,
                generation_config=generation_config
            )
            return response.text
        except Exception as e:
            self.logger.error(f"Geminiç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise

@dataclass
class ResponseVariant:
    """å¿œç­”ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³"""
    id: str
    prompt_template: str
    temperature_range: tuple[float, float]
    weight: float = 1.0
    conditions: Dict[str, Any] = None

class FlexibleAIService:
    """æŸ”è»Ÿæ€§ã®é«˜ã„AIã‚µãƒ¼ãƒ“ã‚¹"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
        self.providers = self._initialize_providers()
        self.active_provider = self._select_best_provider()

        # å¿œç­”ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç®¡ç†
        self.response_variants = self._initialize_response_variants()

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†
        self.context_manager = AIContextManager()

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®š
        self.user_settings: Dict[str, Dict[str, Any]] = {}

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ç®¡ç†
        self.rate_limiter = RateLimiter()

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.response_cache: Dict[str, Dict[str, Any]] = {}
        self.cache_lock = threading.Lock()

        # ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰è¨­å®š
        self.mock_mode = os.getenv('MOCK_MODE', 'false').lower() == 'true'

        self.logger.info("Flexible AIã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
        if self.mock_mode:
            self.logger.info("ğŸ§ª ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™")

    def _initialize_providers(self) -> Dict[str, BaseAIProvider]:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–"""
        providers = {}

        # Geminiè¨­å®š
        gemini_config = AIProviderConfig(
            name="gemini",
            api_key_env="GEMINI_API_KEY",
            base_url="https://generativelanguage.googleapis.com",
            model_name="gemini-2.5-flash-preview-05-20",
            temperature=0.7,
            max_tokens=4000
        )

        try:
            providers["gemini"] = GeminiProvider(gemini_config)
            self.logger.info("Geminiãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
        except Exception as e:
            self.logger.warning(f"Geminiãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—: {str(e)}")
            # ãƒ¢ãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½œæˆ
            providers["gemini"] = self._create_mock_provider()

        # ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚‚ã“ã“ã«è¿½åŠ å¯èƒ½
        # providers["openai"] = OpenAIProvider(...)
        # providers["claude"] = ClaudeProvider(...)

        return providers

    def _select_best_provider(self) -> Optional[BaseAIProvider]:
        """æœ€é©ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’é¸æŠ"""
        if not self.providers:
            # ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ãƒ¢ãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
            if self.mock_mode:
                return self._create_mock_provider()
            return None

        # åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‹ã‚‰é¸æŠï¼ˆå„ªå…ˆåº¦é †ï¼‰
        priorities = ["gemini", "openai", "claude"]

        for provider_name in priorities:
            if provider_name in self.providers:
                provider = self.providers[provider_name]
                if self._test_provider(provider):
                    return provider

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦æœ€åˆã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
        return list(self.providers.values())[0] if self.providers else None

    def _test_provider(self, provider: BaseAIProvider) -> bool:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å‹•ä½œç¢ºèª"""
        try:
            # ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            test_prompt = "Hello, are you working?"
            response = provider.generate_sync(test_prompt)
            return len(response) > 0
        except Exception as e:
            self.logger.warning(f"ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆå¤±æ•—: {str(e)}")
            return False

    def _initialize_response_variants(self) -> Dict[str, List[ResponseVariant]]:
        """å¿œç­”ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åˆæœŸåŒ–"""
        variants = {
            "general": [
                ResponseVariant(
                    id="creative",
                    prompt_template="{base_prompt}\n\nå‰µé€ çš„ã«ã€ç‹¬è‡ªã®è¦–ç‚¹ã§ç­”ãˆã¦ãã ã•ã„ã€‚",
                    temperature_range=(0.8, 1.0),
                    weight=0.3
                ),
                ResponseVariant(
                    id="balanced",
                    prompt_template="{base_prompt}\n\nãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸã€å½¹ç«‹ã¤å›ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚",
                    temperature_range=(0.5, 0.7),
                    weight=0.5
                ),
                ResponseVariant(
                    id="precise",
                    prompt_template="{base_prompt}\n\næ­£ç¢ºã§ã€äº‹å®Ÿã«åŸºã¥ã„ãŸå›ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚",
                    temperature_range=(0.1, 0.3),
                    weight=0.2
                )
            ],
            "casual": [
                ResponseVariant(
                    id="friendly",
                    prompt_template="{base_prompt}\n\nãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§ã€è¦ªã—ã¿ã‚„ã™ã„å£èª¿ã§ç­”ãˆã¦ãã ã•ã„ã€‚",
                    temperature_range=(0.7, 0.9),
                    weight=0.6
                ),
                ResponseVariant(
                    id="humorous",
                    prompt_template="{base_prompt}\n\nãƒ¦ãƒ¼ãƒ¢ã‚¢ã‚’äº¤ãˆã¤ã¤ã€å½¹ç«‹ã¤å›ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚",
                    temperature_range=(0.8, 1.0),
                    weight=0.4
                )
            ],
            "technical": [
                ResponseVariant(
                    id="detailed",
                    prompt_template="{base_prompt}\n\næŠ€è¡“çš„ãªè©³ç´°ã‚’äº¤ãˆã¦ã€è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
                    temperature_range=(0.3, 0.6),
                    weight=0.7
                ),
                ResponseVariant(
                    id="concise",
                    prompt_template="{base_prompt}\n\nç°¡æ½”ã«ã€é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã ã‘ã‚’ã¾ã¨ã‚ã¦ç­”ãˆã¦ãã ã•ã„ã€‚",
                    temperature_range=(0.2, 0.4),
                    weight=0.3
                )
            ]
        }
        return variants

    def generate_flexible_response_sync(
        self,
        prompt: str,
        user_id: str = "default",
        context: Optional[Dict[str, Any]] = None,
        response_style: str = "balanced",
        force_refresh: bool = False
    ) -> str:
        """
        åŒæœŸç‰ˆã®æŸ”è»Ÿãªå¿œç­”ç”Ÿæˆ

        Args:
            prompt: ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            context: è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            response_style: å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«
            force_refresh: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸå¿œç­”
        """
        try:
            # ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
            if self.mock_mode:
                return self._generate_mock_response(prompt, user_id, context, response_style)

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            cache_key = self._generate_cache_key(prompt, user_id, response_style)
            if not force_refresh:
                cached_response = self._get_cached_response(cache_key)
                if cached_response:
                    return cached_response

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
            if not self.rate_limiter.check_limit(user_id):
                return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ç¾åœ¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒé›†ä¸­ã—ã¦ã„ã¾ã™ã€‚å°‘ã—å¾…ã£ã¦ã‹ã‚‰ãŠè©¦ã—ãã ã•ã„ã€‚"

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆçµ±åˆ
            enhanced_prompt = self._enhance_prompt_with_context_sync(prompt, user_id, context)

            # å¿œç­”ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³é¸æŠ
            variant = self._select_response_variant(response_style, context)

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Œæˆ
            final_prompt = variant.prompt_template.format(base_prompt=enhanced_prompt)

            # æ¸©åº¦è¨­å®š
            # ResponseVariant.temperature_range ã‚’æ­£ã—ãå‚ç…§
            temp_min, temp_max = (variant.temperature_range if isinstance(getattr(variant, 'temperature_range', None), tuple)
                                  else (0.5, 0.7))
            temperature = random.uniform(temp_min, temp_max)

            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠ
            provider = self.active_provider
            if not provider:
                return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚AIã‚µãƒ¼ãƒ“ã‚¹ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚"

            # ç”Ÿæˆå®Ÿè¡Œ
            response = provider.generate_sync(
                final_prompt,
                temperature=temperature,
                max_tokens=self._calculate_max_tokens(prompt)
            )

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            self._cache_response(cache_key, response)

            # ä½¿ç”¨çŠ¶æ³è¨˜éŒ²
            self._record_usage(user_id, "generate_flexible_response")

            return response

        except Exception as e:
            self.logger.error(f"æŸ”è»Ÿå¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

    def _generate_mock_response(self, prompt: str, user_id: str, context: Optional[Dict[str, Any]], response_style: str) -> str:
        """ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§ã®å¿œç­”ç”Ÿæˆ"""
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ã„ã¦é©åˆ‡ãªãƒ¢ãƒƒã‚¯å¿œç­”ã‚’ç”Ÿæˆ
        prompt_lower = prompt.lower()

        if "å¤©æ°—" in prompt_lower:
            return f"ğŸŒ¤ï¸ æ±äº¬ã®ç¾åœ¨ã®å¤©æ°—ã¯æ™´ã‚Œã§ã€æ°—æ¸©ã¯22â„ƒã§ã™ã€‚ä»Šæ—¥ã¯å¿«é©ãªä¸€æ—¥ã«ãªã‚Šãã†ã§ã™ã€‚"
        elif "é€šçŸ¥" in prompt_lower:
            return "âœ… é€šçŸ¥ã‚’è¨­å®šã—ã¾ã—ãŸã€‚æŒ‡å®šã•ã‚ŒãŸæ™‚é–“ã«ãƒªãƒã‚¤ãƒ³ãƒ‰ã‚’ãŠé€ã‚Šã—ã¾ã™ã€‚"
        elif "æ¤œç´¢" in prompt_lower:
            return "ğŸ” æ¤œç´¢çµæœã‚’è¡¨ç¤ºã—ã¾ã™ã€‚é–¢é€£ã™ã‚‹æƒ…å ±ã‚’ã„ãã¤ã‹è¦‹ã¤ã‘ã¾ã—ãŸã€‚"
        elif "ã“ã‚“ã«ã¡ã¯" in prompt_lower or "å§‹ã‚ã¾ã—ã¦" in prompt_lower:
            return "ã“ã‚“ã«ã¡ã¯ï¼ç§ã¯AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»Šæ—¥ã¯ã©ã®ã‚ˆã†ãªãŠæ‰‹ä¼ã„ãŒã§ãã¾ã™ã‹ï¼Ÿ"
        elif "ã‚ã‚ŠãŒã¨ã†" in prompt_lower:
            return "ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼ãŠå½¹ã«ç«‹ã¦ã¦å¬‰ã—ã„ã§ã™ã€‚ä½•ã‹ä»–ã«è³ªå•ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
        elif "ã‚¨ãƒ©ãƒ¼" in prompt_lower or "å•é¡Œ" in prompt_lower:
            return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚å•é¡ŒãŒç™ºç”Ÿã—ãŸã‚ˆã†ã§ã™ã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ"
        else:
            # æ±ç”¨çš„ãªå¿œç­”
            return f"ã€Œ{prompt[:50]}...ã€ã«ã¤ã„ã¦ã®ãŠå•ã„åˆã‚ã›ã§ã™ã­ã€‚é©åˆ‡ãªå¯¾å¿œã‚’å–ã‚‰ã›ã¦ã„ãŸã ãã¾ã™ã€‚"

    def _select_best_provider(self) -> Optional[BaseAIProvider]:
        """æœ€é©ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’é¸æŠ"""
        if not self.providers:
            # ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ãƒ¢ãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
            if self.mock_mode:
                return self._create_mock_provider()
            return None

        # åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‹ã‚‰é¸æŠï¼ˆå„ªå…ˆåº¦é †ï¼‰
        priorities = ["gemini", "openai", "claude"]

        for provider_name in priorities:
            if provider_name in self.providers:
                provider = self.providers[provider_name]
                if self._test_provider(provider):
                    return provider

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦æœ€åˆã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
        return list(self.providers.values())[0] if self.providers else None

    def _create_mock_provider(self) -> BaseAIProvider:
        """ãƒ¢ãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½œæˆ"""
        class MockProvider:
            def __init__(self):
                self.config = AIProviderConfig(
                    name="mock",
                    api_key_env="MOCK_API_KEY",
                    base_url="mock://localhost",
                    model_name="mock-model",
                    temperature=0.7,
                    max_tokens=1000
                )

            async def generate_async(self, prompt: str, **kwargs) -> str:
                return self._generate_mock_response(prompt, **kwargs)

            def generate_sync(self, prompt: str, **kwargs) -> str:
                return self._generate_mock_response(prompt, **kwargs)

            def _generate_mock_response(self, prompt: str, **kwargs) -> str:
                """ãƒ¢ãƒƒã‚¯å¿œç­”ç”Ÿæˆ"""
                prompt_lower = prompt.lower()

                if "å¤©æ°—" in prompt_lower:
                    return "ç¾åœ¨ã®å¤©æ°—ã¯æ™´ã‚Œã§ã€æ°—æ¸©ã¯22â„ƒã§ã™ã€‚"
                elif "é€šçŸ¥" in prompt_lower:
                    return "é€šçŸ¥ã‚’è¨­å®šã—ã¾ã—ãŸã€‚"
                elif "æ¤œç´¢" in prompt_lower:
                    return "æ¤œç´¢çµæœã‚’è¡¨ç¤ºã—ã¾ã™ã€‚"
                else:
                    return f"ãƒ¢ãƒƒã‚¯å¿œç­”: {prompt[:100]}..."

        return MockProvider()

    def _generate_cache_key(self, prompt: str, user_id: str, style: str) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
        content = f"{prompt}:{user_id}:{style}:{datetime.now().strftime('%Y%m%d%H')}"
        return hashlib.md5(content.encode()).hexdigest()

    def _get_cached_response(self, cache_key: str) -> Optional[str]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å¿œç­”ã‚’å–å¾—"""
        with self.cache_lock:
            if cache_key in self.response_cache:
                cached = self.response_cache[cache_key]
                if datetime.now() - cached['timestamp'] < timedelta(hours=1):
                    return cached['response']
                else:
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœŸé™åˆ‡ã‚Œ
                    del self.response_cache[cache_key]
        return None

    def _cache_response(self, cache_key: str, response: str):
        """å¿œç­”ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        with self.cache_lock:
            self.response_cache[cache_key] = {
                'response': response,
                'timestamp': datetime.now()
            }

    def _enhance_prompt_with_context_sync(self, base_prompt: str, user_id: str, context: Optional[Dict[str, Any]] = None) -> str:
        """åŒæœŸç‰ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆçµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ"""
        # éåŒæœŸç‰ˆã‚’å‘¼ã³å‡ºã—
        import asyncio
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(self._enhance_prompt_with_context(base_prompt, user_id, context))
        finally:
            loop.close()

    async def _enhance_prompt_with_context(self, base_prompt: str, user_id: str, context: Optional[Dict[str, Any]] = None) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ±åˆã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ"""
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šå–å¾—
        user_settings = self.user_settings.get(user_id, {})

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§æ–‡è„ˆå–å¾—
        conversation_context = await self.context_manager.get_context(user_id)

        # åˆ©ç”¨å¯èƒ½ãªAIæ©Ÿèƒ½ã‚’å–å¾—
        available_functions = self._get_available_ai_functions()

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¼·åŒ–
        enhancements = []

        # ä¼šè©±æ–‡è„ˆã®è¿½åŠ 
        if conversation_context:
            enhancements.append(f"ä¼šè©±ã®æ–‡è„ˆ: {conversation_context}")

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã®è¿½åŠ 
        if user_settings.get('preferences'):
            enhancements.append(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿: {user_settings['preferences']}")

        # è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è¿½åŠ 
        if context:
            enhancements.append(f"è¿½åŠ æƒ…å ±: {context}")

        # åˆ©ç”¨å¯èƒ½ãªAIæ©Ÿèƒ½ã®è¿½åŠ 
        if available_functions:
            function_list = "\n".join(f"- {func['name']}: {func['description']}" for func in available_functions)
            enhancements.append(f"åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½:\n{function_list}")

        # æ™‚é–“å¸¯ã«ã‚ˆã‚‹èª¿æ•´
        hour = datetime.now().hour
        if 6 <= hour < 12:
            enhancements.append("æœã®æ™‚é–“å¸¯ãªã®ã§ã€å…ƒæ°—ã§ãƒã‚¸ãƒ†ã‚£ãƒ–ãªãƒˆãƒ¼ãƒ³ã§ç­”ãˆã¦ãã ã•ã„ã€‚")
        elif 18 <= hour < 24:
            enhancements.append("å¤œã®æ™‚é–“å¸¯ãªã®ã§ã€ãƒªãƒ©ãƒƒã‚¯ã‚¹ã—ãŸãƒˆãƒ¼ãƒ³ã§ç­”ãˆã¦ãã ã•ã„ã€‚")

        if enhancements:
            enhanced_prompt = f"{base_prompt}\n\nè¿½åŠ ã®æŒ‡ç¤º:\n" + "\n".join(f"- {e}" for e in enhancements)
            return enhanced_prompt

        return base_prompt

    def _select_response_variant(self, style: str, context: Optional[Dict[str, Any]] = None) -> ResponseVariant:
        """å¿œç­”ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é¸æŠ"""
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨
        variants = self.response_variants.get("general", [])

        # ã‚¹ã‚¿ã‚¤ãƒ«æŒ‡å®šãŒã‚ã‚‹å ´åˆ
        if style in self.response_variants:
            variants = self.response_variants[style]

        # æ–‡è„ˆã«åŸºã¥ã„ã¦é‡ã¿ä»˜ã‘
        if context:
            # æŠ€è¡“çš„ãªå†…å®¹ã®å ´åˆã€technicalãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å„ªå…ˆ
            if context.get('is_technical', False):
                technical_variants = self.response_variants.get("technical", [])
                if technical_variants:
                    variants = technical_variants

        # é‡ã¿ä»˜ã‘ã«ã‚ˆã‚‹é¸æŠ
        total_weight = sum(v.weight for v in variants)
        if total_weight == 0:
            return variants[0] if variants else ResponseVariant("default", "{base_prompt}", (0.7, 0.7))

        r = random.random() * total_weight
        current_weight = 0
        for variant in variants:
            current_weight += variant.weight
            if r <= current_weight:
                return variant

    def _get_available_ai_functions(self) -> List[Dict[str, Any]]:
        """åˆ©ç”¨å¯èƒ½ãªAIæ©Ÿèƒ½ã‚’å–å¾—"""
        try:
            from .ai_function_plugin import ai_function_registry
            functions = ai_function_registry.list_functions()

            return [
                {
                    "name": func.name,
                    "description": func.description,
                    "trigger_keywords": func.trigger_keywords[:3],  # æœ€åˆã®3ã¤ã ã‘
                    "category": self._categorize_function(func.name)
                }
                for func in functions
            ]
        except Exception as e:
            self.logger.warning(f"AIæ©Ÿèƒ½å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []

    def _categorize_function(self, function_name: str) -> str:
        """æ©Ÿèƒ½ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        name_lower = function_name.lower()

        if any(keyword in name_lower for keyword in ['notification', 'é€šçŸ¥', 'remind']):
            return 'é€šçŸ¥'
        elif any(keyword in name_lower for keyword in ['weather', 'å¤©æ°—']):
            return 'å¤©æ°—'
        elif any(keyword in name_lower for keyword in ['search', 'æ¤œç´¢']):
            return 'æ¤œç´¢'
        elif any(keyword in name_lower for keyword in ['task', 'ã‚¿ã‚¹ã‚¯', 'auto']):
            return 'ã‚¿ã‚¹ã‚¯'
        elif any(keyword in name_lower for keyword in ['math', 'è¨ˆç®—']):
            return 'è¨ˆç®—'
        elif any(keyword in name_lower for keyword in ['translate', 'ç¿»è¨³']):
            return 'ç¿»è¨³'
        else:
            return 'ãã®ä»–'

    def _calculate_max_tokens(self, prompt: str) -> int:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ã«åŸºã¥ã„ã¦æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—"""
        prompt_length = len(prompt.split())
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒé•·ã„å ´åˆã¯å¿œç­”ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ¸›ã‚‰ã™
        if prompt_length > 1000:
            return 1000
        elif prompt_length > 500:
            return 2000
        else:
            return 4000

    def _record_usage(self, user_id: str, action: str):
        """ä½¿ç”¨çŠ¶æ³ã‚’è¨˜éŒ²"""
        # å®Ÿéš›ã®ä½¿ç”¨çŠ¶æ³è¨˜éŒ²ï¼ˆå¿…è¦ã«å¿œã˜ã¦å®Ÿè£…ï¼‰
        pass

    def get_user_settings(self, user_id: str) -> Dict[str, Any]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã‚’å–å¾—"""
        return self.user_settings.get(user_id, {})

    def update_user_settings(self, user_id: str, settings: Dict[str, Any]):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã‚’æ›´æ–°"""
        if user_id not in self.user_settings:
            self.user_settings[user_id] = {}
        self.user_settings[user_id].update(settings)

    def add_response_variant(self, category: str, variant: ResponseVariant):
        """å¿œç­”ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ """
        if category not in self.response_variants:
            self.response_variants[category] = []
        self.response_variants[category].append(variant)

    async def generate_flexible_response(
        self,
        prompt: str,
        user_id: str = "default",
        context: Optional[Dict[str, Any]] = None,
        response_style: str = "balanced",
        force_refresh: bool = False
    ) -> str:
        """
        éåŒæœŸç‰ˆã®æŸ”è»Ÿãªå¿œç­”ç”Ÿæˆ

        Args:
            prompt: ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            context: è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            response_style: å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«
            force_refresh: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸå¿œç­”
        """
        try:
            # ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
            if self.mock_mode:
                return self._generate_mock_response(prompt, user_id, context, response_style)

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            cache_key = self._generate_cache_key(prompt, user_id, response_style)
            if not force_refresh:
                cached_response = self._get_cached_response(cache_key)
                if cached_response:
                    return cached_response

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
            if not self.rate_limiter.check_limit(user_id):
                return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ç¾åœ¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒé›†ä¸­ã—ã¦ã„ã¾ã™ã€‚å°‘ã—å¾…ã£ã¦ã‹ã‚‰ãŠè©¦ã—ãã ã•ã„ã€‚"

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆçµ±åˆ
            enhanced_prompt = await self._enhance_prompt_with_context(prompt, user_id, context)

            # å¿œç­”ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³é¸æŠ
            variant = self._select_response_variant(response_style, context)

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Œæˆ
            final_prompt = variant.prompt_template.format(base_prompt=enhanced_prompt)

            # æ¸©åº¦è¨­å®š
            import random
            temp_min, temp_max = (variant.temperature_range if isinstance(getattr(variant, 'temperature_range', None), tuple)
                                  else (0.5, 0.7))
            temperature = random.uniform(temp_min, temp_max)

            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠ
            provider = self.active_provider
            if not provider:
                return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚AIã‚µãƒ¼ãƒ“ã‚¹ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚"

            # ç”Ÿæˆå®Ÿè¡Œ
            response = await provider.generate_async(
                final_prompt,
                temperature=temperature,
                max_tokens=self._calculate_max_tokens(prompt)
            )

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            self._cache_response(cache_key, response)

            # ä½¿ç”¨çŠ¶æ³è¨˜éŒ²
            self._record_usage(user_id, "generate_flexible_response")

            return response

        except Exception as e:
            self.logger.error(f"æŸ”è»Ÿå¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

class RateLimiter:
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ç®¡ç†"""

    def __init__(self, max_requests_per_minute: int = 60):
        self.max_requests = max_requests_per_minute
        self.user_requests: Dict[str, List[datetime]] = {}
        self.lock = threading.Lock()

    def check_limit(self, user_id: str) -> bool:
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯"""
        now = datetime.now()

        with self.lock:
            if user_id not in self.user_requests:
                self.user_requests[user_id] = []

            # 1åˆ†ä»¥å†…ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            self.user_requests[user_id] = [
                req_time for req_time in self.user_requests[user_id]
                if now - req_time < timedelta(minutes=1)
            ]

            if len(self.user_requests[user_id]) >= self.max_requests:
                return False

            self.user_requests[user_id].append(now)
            return True

class AIContextManager:
    """AIã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†"""

    def __init__(self):
        self.contexts: Dict[str, Dict[str, Any]] = {}
        self.lock = threading.Lock()

    async def get_context(self, user_id: str) -> Optional[str]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
        with self.lock:
            if user_id in self.contexts:
                return self.contexts[user_id].get('conversation_summary')
        return None

    def update_context(self, user_id: str, context: Dict[str, Any]):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ›´æ–°"""
        with self.lock:
            if user_id not in self.contexts:
                self.contexts[user_id] = {}
            self.contexts[user_id].update(context)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
flexible_ai_service = FlexibleAIService()
